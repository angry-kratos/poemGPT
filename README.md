# poemGPT
 A Decoder-only transformer trained on poems
- The Model is based on Andrej Kaparthy's Nano GPT
- The Model is a Decoder only Model which technically 'Blabbers" words based on the poems it has been trained on
  ![WhatsApp Image 2023-07-19 at 02 29 41](https://github.com/angry-kratos/poemGPT/assets/99096395/4ed38a7e-6e40-4e0d-8238-f259e5d14430)
This is the basic architecture

- In the code u can find the use of dropout layers but while experimenting with different training parameters it was observed that not having a dropout layer was resulting in better performance
- The results are in constraint with available compute on my local machine 
